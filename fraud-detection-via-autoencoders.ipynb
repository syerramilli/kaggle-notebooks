{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThe main task in this dataset is to predict which of the transactions are fraudulent, based on several numerical measures. A major challenge is the large class imbalance in the data - only 0.17% of the transactions are fraudulent. It will be difficult to train classification models with good discriminative power. \n\nIn this notebook, we take an alternative approach by employing an autoencoder-based anomaly detection method to classify transactions. The underlying assumption is that if the features carry some relevance for predicting fraud, then fraudulent transactions are likely to be outliers within the feature space when compared to non-fraudulent transactions. To implement this approach, we train the autoencoder only on the non-fraudulent transactions. Subsequently, the reconstruction error derived from this autoencoder serves as the foundation for constructing a decision function to classify transactions as fraudulent or not. \n\nGiven the dataset's substantial class imbalance, we will evaluate model performance using the area under the precision-recall curve, which is a suitable metric for assessing model quality in such imbalanced scenarios.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc\n\nimport joblib\n\nfrom typing import Dict, Optional, List, Tuple\nfrom numbers import Number\n\nplt.style.use(\"ggplot\")\nplt.rcParams.update(**{'figure.dpi':150})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-27T23:58:06.435624Z","iopub.execute_input":"2023-09-27T23:58:06.436742Z","iopub.status.idle":"2023-09-27T23:58:20.639122Z","shell.execute_reply.started":"2023-09-27T23:58:06.436684Z","shell.execute_reply":"2023-09-27T23:58:20.637664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data\n\n**Note**: We drop the `Time` feature since we don't know how to interpret it. ","metadata":{}},{"cell_type":"code","source":"raw_df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv').drop('Time', axis=1)\nraw_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:27.418157Z","iopub.execute_input":"2023-09-28T00:07:27.418663Z","iopub.status.idle":"2023-09-28T00:07:31.939657Z","shell.execute_reply.started":"2023-09-28T00:07:27.418620Z","shell.execute_reply":"2023-09-28T00:07:31.937872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of observations: {raw_df.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:31.942322Z","iopub.execute_input":"2023-09-28T00:07:31.942893Z","iopub.status.idle":"2023-09-28T00:07:31.950377Z","shell.execute_reply.started":"2023-09-28T00:07:31.942839Z","shell.execute_reply":"2023-09-28T00:07:31.948887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neg, pos = np.bincount(raw_df['Class'])\ntotal = neg + pos\nprint(f'Number of positive observations: {pos} ({100*pos / total:.2f}% of total)')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:31.952434Z","iopub.execute_input":"2023-09-28T00:07:31.952973Z","iopub.status.idle":"2023-09-28T00:07:31.968632Z","shell.execute_reply.started":"2023-09-28T00:07:31.952926Z","shell.execute_reply":"2023-09-28T00:07:31.966904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating training, validation and test splits","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(raw_df, test_size=0.2, random_state=1, stratify=raw_df['Class'])\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=2, stratify=train_df['Class'])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:32.996536Z","iopub.execute_input":"2023-09-28T00:07:32.998183Z","iopub.status.idle":"2023-09-28T00:07:33.385708Z","shell.execute_reply.started":"2023-09-28T00:07:32.998134Z","shell.execute_reply":"2023-09-28T00:07:33.384092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df.pop('Class').values\ny_val = val_df.pop('Class').values\ny_test = test_df.pop('Class').values","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:33.577111Z","iopub.execute_input":"2023-09-28T00:07:33.577610Z","iopub.status.idle":"2023-09-28T00:07:33.586498Z","shell.execute_reply.started":"2023-09-28T00:07:33.577573Z","shell.execute_reply":"2023-09-28T00:07:33.584921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tag, labels in zip(['training', 'validation', 'test'], [y_train, y_val, y_test]):\n    print(f'Percentage of positive class observations in {tag} set: {labels.mean()*100:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:33.976442Z","iopub.execute_input":"2023-09-28T00:07:33.976976Z","iopub.status.idle":"2023-09-28T00:07:33.986509Z","shell.execute_reply.started":"2023-09-28T00:07:33.976937Z","shell.execute_reply":"2023-09-28T00:07:33.984910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA","metadata":{}},{"cell_type":"code","source":"def filter_greater_than(series:pd.Series,threshold:Number) -> pd.Series:\n    '''\n    Returns series elements greater than threshold. This funtion can be\n    used with the .pipe methods\n    '''\n    return series[series>threshold]\n\n\ndef get_missing_percentage(df:pd.DataFrame) -> pd.Series:\n    '''\n    Returns the percentages of missing values for columns \n    in `df`that have atleast one missing entry\n    '''\n    \n    return (\n        (df.isnull().sum()/df.shape[0]*100)\n        .sort_values(ascending=False)\n        .pipe(filter_greater_than,threshold=0)\n        .round(3)\n    )\n\n\nfor tag, df in zip(['training', 'validation', 'test'], [train_df, val_df, test_df]):\n    print(f'Percentage of missing entries per column in {tag} set (if any):')\n    print(get_missing_percentage(df))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:35.566292Z","iopub.execute_input":"2023-09-28T00:07:35.566788Z","iopub.status.idle":"2023-09-28T00:07:35.602810Z","shell.execute_reply.started":"2023-09-28T00:07:35.566753Z","shell.execute_reply":"2023-09-28T00:07:35.601294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_columns = train_df.skew()\npos_skew = {}\nneg_skew = {}\n\nfor column, skew in skew_columns.items():\n    if skew > 1:\n        pos_skew[column] = skew\n    elif skew < -1:\n        neg_skew[column] = skew\n        \nprint(f'Number of columns that are positively skewed: {len(pos_skew)}')\nprint(f'Number of columns that are negatively skewed: {len(neg_skew)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:36.929087Z","iopub.execute_input":"2023-09-28T00:07:36.929625Z","iopub.status.idle":"2023-09-28T00:07:36.999718Z","shell.execute_reply.started":"2023-09-28T00:07:36.929578Z","shell.execute_reply":"2023-09-28T00:07:36.998290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For simplicity, we preprocess all the columns through the `QuantileTransformer` in scikit-learn, so that the transformed features are (roughly) normally distributed across the training set.","metadata":{}},{"cell_type":"code","source":"qt_transform = QuantileTransformer(output_distribution='normal')\nX_train = qt_transform.fit_transform(train_df)\nX_val = qt_transform.transform(val_df)\nX_test = qt_transform.transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:07:59.132475Z","iopub.execute_input":"2023-09-28T00:07:59.132962Z","iopub.status.idle":"2023-09-28T00:08:01.611592Z","shell.execute_reply.started":"2023-09-28T00:07:59.132922Z","shell.execute_reply":"2023-09-28T00:08:01.610472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder architecture","metadata":{}},{"cell_type":"code","source":"def encoder(inputs, params={}):\n    x = inputs\n    for i in range(params.get('n_hidden_encoder', 2)):\n        x = keras.layers.Dense(params.get(f'hsize_encoder{i}', 64//2**i), activation=None)(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n        x = keras.layers.Dropout(params.get(f'dropout_encoder{i}', 0.05))(x)\n    \n    return x\n\ndef decoder(inputs, input_dim, params={}):\n    x = inputs\n    n_hidden_decoder = params.get('n_hidden_decoder', 2)\n    for i in range(n_hidden_decoder):\n        x = keras.layers.Dense(params.get(f'hsize_decoder{i}', 64//2**(n_hidden_decoder-i-1)), activation=None)(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n        x = keras.layers.Dropout(params.get(f'dropout_decoder{i}', 0.05))(x)\n    \n    # output\n    x = keras.layers.Dense(input_dim, activation=None)(x)\n    return x\n\ndef bottleneck_layer(inputs, params={}):\n    x = keras.layers.Dense(params.get(f'hsize_bottleneck', 16), activation=None)(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU(name='bottleneck')(x)\n    \n    return x\n\n\ndef dense_autoencoder(input_dim, params={}):\n    inputs = keras.Input(shape=(input_dim,))\n    encoder_output = encoder(inputs, params)\n    bottleneck_output = bottleneck_layer(encoder_output, params)\n    decoder_output = decoder(bottleneck_output, input_dim, params)\n    \n    model = keras.Model(inputs=inputs, outputs=decoder_output)\n    \n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            learning_rate=params.get('learning_rate', 1e-3),\n        ),\n        loss='mean_squared_error'\n    )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:08:01.618207Z","iopub.execute_input":"2023-09-28T00:08:01.618722Z","iopub.status.idle":"2023-09-28T00:08:01.638955Z","shell.execute_reply.started":"2023-09-28T00:08:01.618684Z","shell.execute_reply":"2023-09-28T00:08:01.636900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nae_model = dense_autoencoder(X_train.shape[1])\nae_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:08:01.640963Z","iopub.execute_input":"2023-09-28T00:08:01.641527Z","iopub.status.idle":"2023-09-28T00:08:02.036362Z","shell.execute_reply.started":"2023-09-28T00:08:01.641473Z","shell.execute_reply":"2023-09-28T00:08:02.034912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the autoencoder\n\nAs previously mentioned in the introduction, our approach involves training the autoencoder exclusively on the non-fraudulent transactions. The primary goal is to minimize the reconstruction error generated by the network. This strategy is grounded in the assumption that non-fraudulent transactions provide a reliable representation of typical, legitimate data patterns.\n\nTo optimize the training process and ensure the best model performance, we employ two key techniques:\n\n1. Early Stopping: Early stopping is a mechanism used during training to prevent overfitting. By monitoring the reconstruction error on the validation set (comprising non-fraudulent transactions), we can halt training when the error starts to increase or stagnate. This ensures that we stop training before the model becomes overly specialized to the training data, resulting in better generalization to unseen data.\n2. Learning Rate Scheduler: We use a learning rate scheduler that adjusts the learning rate during training based on the reconstruction error of the non-fraudulent transactions in the validation set. When the error plateaus or increases, the learning rate is halved.\n\nBy incorporating these strategies, we aim to train an autoencoder that effectively captures the underlying patterns of legitimate transactions. This approach is crucial in building a reliable anomaly detection system, where the autoencoder's reconstruction error serves as a key metric for identifying suspicious or fraudulent activities.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 128\n    \n    \n# callbacks - reduce lr on plateau and early stopping\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode='min',\n    verbose=True,\n    patience=15,\n    restore_best_weights=True\n)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    mode='min',\n    verbose=True,\n    patience=5,\n    factor= 0.5,\n    min_lr = 1e-5\n)\n\nhistory = ae_model.fit(\n    X_train[y_train==0,:],\n    X_train[y_train==0,:],\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=2,\n    callbacks = [early_stopping, reduce_lr], \n    validation_data=(X_val[y_val==0], X_val[y_val==0])\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-28T00:08:38.872940Z","iopub.execute_input":"2023-09-28T00:08:38.873421Z","iopub.status.idle":"2023-09-28T00:20:27.830843Z","shell.execute_reply.started":"2023-09-28T00:08:38.873385Z","shell.execute_reply":"2023-09-28T00:20:27.829512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 1, figsize=(4,3))\nmetric = 'loss'\n_ = axs.plot(history.epoch, history.history[f'{metric}'], label='Training')\n_ = axs.plot(history.epoch, history.history[f'val_{metric}'], label='Validation')                 \n_ = axs.legend()\n_ = axs.set_xlabel('Epoch')\n_ = axs.set_ylabel('Reconstruction loss')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:20:27.833030Z","iopub.execute_input":"2023-09-28T00:20:27.833432Z","iopub.status.idle":"2023-09-28T00:20:28.160602Z","shell.execute_reply.started":"2023-09-28T00:20:27.833397Z","shell.execute_reply":"2023-09-28T00:20:28.158823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reconstruction loss across classes\n\nFor each observation, we define the reconstruction loss as the sum of the squared differences between its features and uts corresponding reconstructed output from the autoencoder.","metadata":{}},{"cell_type":"code","source":"def get_reconst_loss(X):\n    X_reconst = ae_model.predict(X, batch_size=256)\n    return (\n        ((X-X_reconst)**2).sum(axis=1)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:21:14.536595Z","iopub.execute_input":"2023-09-28T00:21:14.537201Z","iopub.status.idle":"2023-09-28T00:21:14.545717Z","shell.execute_reply.started":"2023-09-28T00:21:14.537161Z","shell.execute_reply":"2023-09-28T00:21:14.543722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we have generated a histogram plotting the log-transformed values of (1 + reconstruction loss) for the two distinct classes. The y-axis represents the density within each class. As anticipated, the fraudulent transactions tend to exhibit significantly higher reconstruction losses compared to the non-fraudulent ones. This observation underscores the utility of the reconstruction loss as a reliable criterion for classifying transactions as either fraudulent or legitimate.","metadata":{}},{"cell_type":"code","source":"reconst_train = get_reconst_loss(X_train)\n\nfig, ax = plt.subplots(1,1, figsize=(6,4))\n_ = sns.histplot(\n    x=np.log1p(reconst_train[y_train==0]), bins = 20, stat='density',\n    kde=True,alpha = 0.5, label=0, ax=ax\n)\n_ = sns.histplot(\n    x=np.log1p(reconst_train[y_train==1]), bins = 20, stat='density',\n    kde=True, alpha = 0.75, label=1, ax=ax\n)\n_ = ax.legend()\n_ = ax.set_xlabel('log(1+ Reconstruction Loss)')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:38:28.807771Z","iopub.execute_input":"2023-09-28T00:38:28.808340Z","iopub.status.idle":"2023-09-28T00:38:32.892928Z","shell.execute_reply.started":"2023-09-28T00:38:28.808305Z","shell.execute_reply":"2023-09-28T00:38:32.891304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the performance of the models","metadata":{}},{"cell_type":"code","source":"reconst_val = get_reconst_loss(X_val)\nreconst_test = get_reconst_loss(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:21:40.543073Z","iopub.execute_input":"2023-09-28T00:21:40.543497Z","iopub.status.idle":"2023-09-28T00:21:42.002139Z","shell.execute_reply.started":"2023-09-28T00:21:40.543464Z","shell.execute_reply":"2023-09-28T00:21:41.999627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Area under the precision recall curve\n\nWe can use the reconstruction error to get the precision and recall at various thresholds. The function `pr_auc_score` computes the area under the precision recall curve.","metadata":{}},{"cell_type":"code","source":"def pr_auc_score(labels, predictions):\n    # compute precision recall at several thresholds\n    precision, recall, _ = precision_recall_curve(labels, predictions)\n    \n    return auc(recall, precision)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:22:15.102606Z","iopub.execute_input":"2023-09-28T00:22:15.103074Z","iopub.status.idle":"2023-09-28T00:22:15.110490Z","shell.execute_reply.started":"2023-09-28T00:22:15.103039Z","shell.execute_reply":"2023-09-28T00:22:15.109034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Area under PR curve for training set: {pr_auc_score(y_train, np.log(reconst_train)):.3f}')\nprint(f'Area under PR curve for validation set: {pr_auc_score(y_val, np.log(reconst_val)):.3f}')\nprint(f'Area under PR curve for test set: {pr_auc_score(y_test, np.log(reconst_test)):.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:38:18.602218Z","iopub.execute_input":"2023-09-28T00:38:18.602671Z","iopub.status.idle":"2023-09-28T00:38:18.676382Z","shell.execute_reply.started":"2023-09-28T00:38:18.602639Z","shell.execute_reply":"2023-09-28T00:38:18.674896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Precision recall curves","metadata":{}},{"cell_type":"code","source":"def plot_pr_curve(name, labels, predictions, ax, **kwargs):\n    precision, recall, _ = precision_recall_curve(labels, predictions)\n\n    _ = ax.plot(recall, precision, label=name, linewidth=2, **kwargs)\n    _ = ax.set_ylabel('Precision')\n    _ = ax.set_xlabel('Recall')\n    _ = ax.grid(True)\n    _ = ax.set_aspect('equal')\n    \nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\n\n\nplot_pr_curve(\"Training\", y_train, np.log1p(reconst_train), ax)\nplot_pr_curve(\"Validation\", y_val, np.log1p(reconst_val), ax)\nplot_pr_curve(\"Test\", y_test, np.log1p(reconst_test), ax)\n_ = ax.legend(loc='lower left')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T01:07:16.742177Z","iopub.execute_input":"2023-09-28T01:07:16.742724Z","iopub.status.idle":"2023-09-28T01:07:17.162927Z","shell.execute_reply.started":"2023-09-28T01:07:16.742694Z","shell.execute_reply":"2023-09-28T01:07:17.161657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix\n\nFinally, we evaulate the confusion matrix on the test set, using various thresholds on the log(1+reconstruction loss) to generate the classes.","metadata":{}},{"cell_type":"code","source":"thresholds = [1.5, 2.5, 3.5, 4.5]\n\nfig, axs = plt.subplots(1, len(thresholds), figsize=(4.5*len(thresholds), 4))\nfor i, threshold in enumerate(thresholds):\n    _ = sns.heatmap(confusion_matrix(y_test, np.log1p(reconst_test) > threshold), annot=True, ax=axs[i], fmt='g')\n    _ = axs[i].set_ylabel('Actual')\n    _ = axs[i].set_xlabel('Predicted')\n    _ = axs[i].set_title(f'Confusion matrix @ {threshold}')\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T00:38:52.976736Z","iopub.execute_input":"2023-09-28T00:38:52.977218Z","iopub.status.idle":"2023-09-28T00:38:54.803365Z","shell.execute_reply.started":"2023-09-28T00:38:52.977184Z","shell.execute_reply":"2023-09-28T00:38:54.801719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Further directions\n\nThere are several directions to improve upon this work.\n\n1. Feature selection: The current reconstruction loss function treats all features as equally important. However, it's highly probable that not all features hold the same predictive power. In fact, some features may not even be relevant for our task. Therefore, considering feature selection strategies becomes essential to align the reconstruction task more effectively with the classification task.\n2. Calibration: In our approach, we didn't calculate class probabilities; instead, we relied on thresholds for the reconstruction loss. Enhancements in performance could potentially be achieved through calibration techniques, such as Platt's scaling or isotonic regression. These methods can fine-tune the decision boundaries and improve the model's ability to distinguish between fraudulent and non-fraudulent transactions.\n3. Hyperparameter Tuning: Tuning the autoencoder hyperparameters can potentially improve performance.  It is crucial to emphasize that while tuning, our primary objective should be improving classification, rather than focusing solely on reconstruction quality. ","metadata":{}}]}