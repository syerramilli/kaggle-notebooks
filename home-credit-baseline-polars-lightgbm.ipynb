{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221397c7",
   "metadata": {
    "papermill": {
     "duration": 0.007664,
     "end_time": "2024-02-19T00:43:26.832369",
     "exception": false,
     "start_time": "2024-02-19T00:43:26.824705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building a basline model for the Home Credit Kaggle competition. The goal of this competition is to predict the probability of a client defaulting on a loan. Since the dataset is quite large, we will be using Polars to load and process the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e041da",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:26.849442Z",
     "iopub.status.busy": "2024-02-19T00:43:26.848616Z",
     "iopub.status.idle": "2024-02-19T00:43:33.599396Z",
     "shell.execute_reply": "2024-02-19T00:43:33.598491Z"
    },
    "papermill": {
     "duration": 6.762208,
     "end_time": "2024-02-19T00:43:33.602011",
     "exception": false,
     "start_time": "2024-02-19T00:43:26.839803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from typing import List\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.dpi':150})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17eae3",
   "metadata": {
    "papermill": {
     "duration": 0.006927,
     "end_time": "2024-02-19T00:43:33.616838",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.609911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "\n",
    "1. We will be using the parquet files provided by the competition. The parquet files are stored in the `parquet_files` directory. The `train` and `test` directories contain the training and testing data, respectively. \n",
    "2. We use the Lazy API to read the parquet files using `scan_parquet`. This allows for efficient reading of the data and also allows us to perform operations on the data without loading it into memory until necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb27e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:33.632997Z",
     "iopub.status.busy": "2024-02-19T00:43:33.632351Z",
     "iopub.status.idle": "2024-02-19T00:43:33.636886Z",
     "shell.execute_reply": "2024-02-19T00:43:33.635940Z"
    },
    "papermill": {
     "duration": 0.015086,
     "end_time": "2024-02-19T00:43:33.639281",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.624195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/')\n",
    "TEST_DIR = Path('/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2952406b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:33.655474Z",
     "iopub.status.busy": "2024-02-19T00:43:33.654958Z",
     "iopub.status.idle": "2024-02-19T00:43:33.669083Z",
     "shell.execute_reply": "2024-02-19T00:43:33.668269Z"
    },
    "papermill": {
     "duration": 0.024541,
     "end_time": "2024-02-19T00:43:33.671083",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.646542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    for column in df.columns:\n",
    "        if column in {'case_id', 'WEEK_NUM', 'num_group1', 'num_group2'}:\n",
    "            df = df.with_columns(pl.col(column).cast(pl.Int64))\n",
    "        elif column == 'date_decision' or column[-1] == 'D':\n",
    "            df = df.with_columns(pl.col(column).cast(pl.Date))\n",
    "        elif column[-1] in {'A', 'P'}:\n",
    "            df = df.with_columns(pl.col(column).cast(pl.Float64))\n",
    "        elif column[-1] == 'M':\n",
    "            df = df.with_columns(pl.col(column).cast(pl.String))\n",
    "        \n",
    "    return df    \n",
    "\n",
    "\n",
    "def aggregate_exprs(df: pl.LazyFrame) -> List:\n",
    "    '''\n",
    "    Create aggregate expressions for tables with depth > 1. The\n",
    "    actual aggreation is done elsewhere following a groupby statement.\n",
    "\n",
    "    TODO: Currently only supports max aggregation. Add more expressions.\n",
    "    '''\n",
    "    \n",
    "    exprs = []\n",
    "    for column in df.columns:\n",
    "        if column[-1] in {'D', 'A', 'P'}:\n",
    "            # dates or numerical\n",
    "            exprs.append(\n",
    "                pl.max(column).alias(f'max_{column}')\n",
    "            )\n",
    "        # TODO: add more expressions\n",
    "    \n",
    "    return exprs\n",
    "\n",
    "\n",
    "def read_file(path, depth:int=None) -> pl.LazyFrame:\n",
    "    '''\n",
    "    Read a parquet file and return a polars dataframe. If depth > 1,\n",
    "    then the dataframe is grouped by case_id and aggregated.\n",
    "    '''\n",
    "    df = pl.scan_parquet(path).pipe(correct_dtypes)\n",
    "    \n",
    "    if depth in {1, 2}:\n",
    "        df = df.group_by('case_id').agg(aggregate_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth:int=None) -> pl.LazyFrame:\n",
    "    '''\n",
    "    Read all parquet files matching the regex pattern and \n",
    "    concatenate them into a single dataframe. If depth > 1,\n",
    "    then the dataframe is grouped by case_id and aggregated.\n",
    "    '''\n",
    "\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        chunks.append(\n",
    "            pl.scan_parquet(path).pipe(correct_dtypes)\n",
    "        )\n",
    "\n",
    "    df = pl.concat(chunks, how='vertical_relaxed')\n",
    "    if depth in {1, 2}:\n",
    "        df = df.group_by('case_id').agg(aggregate_exprs(df))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d514134b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:33.686645Z",
     "iopub.status.busy": "2024-02-19T00:43:33.686354Z",
     "iopub.status.idle": "2024-02-19T00:43:33.914097Z",
     "shell.execute_reply": "2024-02-19T00:43:33.912979Z"
    },
    "papermill": {
     "duration": 0.238106,
     "end_time": "2024-02-19T00:43:33.916642",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.678536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credit: https://www.kaggle.com/code/greysky/home-credit-baseline\n",
    "data_stores = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"other_dfs\": [\n",
    "        # depth 0\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "        # depth 1\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "        # depth 2\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ebaca",
   "metadata": {
    "papermill": {
     "duration": 0.007278,
     "end_time": "2024-02-19T00:43:33.931637",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.924359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will now merge the dataframes to create one full training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4955bfef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:33.948185Z",
     "iopub.status.busy": "2024-02-19T00:43:33.947809Z",
     "iopub.status.idle": "2024-02-19T00:43:33.957952Z",
     "shell.execute_reply": "2024-02-19T00:43:33.956887Z"
    },
    "papermill": {
     "duration": 0.020894,
     "end_time": "2024-02-19T00:43:33.960121",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.939227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datediff_col_exprs(date_columns):\n",
    "    exprs = []\n",
    "    for column in date_columns:\n",
    "        exprs.append(\n",
    "            (pl.col(column) - pl.col('date_decision')).dt.total_days().alias(f'days_from_decision_{column}')\n",
    "        )\n",
    "    \n",
    "    return exprs\n",
    "\n",
    "def merge_and_basic_fe(df_base, other_dfs:List[pl.LazyFrame]):\n",
    "    '''\n",
    "    Combine the data and add some preliminary feature engineering\n",
    "    '''\n",
    "    \n",
    "    # add month and weekday columns for decision dates\n",
    "    df_base = (\n",
    "        df_base\n",
    "        # the MONTH seems to the same as date_decision without delimiters\n",
    "        .drop('MONTH') \n",
    "        .with_columns(\n",
    "            MONTH_NUM=pl.col('date_decision').dt.month(),\n",
    "            WEEKDAY_NUM=pl.col('date_decision').dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # merge the other dataframes\n",
    "    for i, df in enumerate(other_dfs):\n",
    "        df_base = df_base.join(df, on='case_id', how='left', suffix=f'_{i}')\n",
    "    \n",
    "    # Create a new column for each date column that represents the number of \n",
    "    # days from the decision date. The orginal date column is then dropped.\n",
    "    schema = df_base.schema\n",
    "    date_columns = [column for column in schema if schema[column] == pl.Date]\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(datediff_col_exprs(date_columns))\n",
    "        .drop(date_columns)\n",
    "        .drop('date_decision')\n",
    "    )\n",
    "    \n",
    "    return df_base.with_columns(pl.col('opencred_647L').cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420e2905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:33.976213Z",
     "iopub.status.busy": "2024-02-19T00:43:33.975906Z",
     "iopub.status.idle": "2024-02-19T00:43:33.989798Z",
     "shell.execute_reply": "2024-02-19T00:43:33.988788Z"
    },
    "papermill": {
     "duration": 0.024646,
     "end_time": "2024-02-19T00:43:33.992133",
     "exception": false,
     "start_time": "2024-02-19T00:43:33.967487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns........: 293\n"
     ]
    }
   ],
   "source": [
    "df_train = merge_and_basic_fe(**data_stores)\n",
    "schema = df_train.schema\n",
    "print(f'Number of columns........: {len(schema)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2a53c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:34.011092Z",
     "iopub.status.busy": "2024-02-19T00:43:34.010696Z",
     "iopub.status.idle": "2024-02-19T00:43:39.747948Z",
     "shell.execute_reply": "2024-02-19T00:43:39.746676Z"
    },
    "papermill": {
     "duration": 5.749768,
     "end_time": "2024-02-19T00:43:39.750345",
     "exception": false,
     "start_time": "2024-02-19T00:43:34.000577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations...: 1526659\n"
     ]
    }
   ],
   "source": [
    "# We haven't actually run anything until this step\n",
    "# We need to read in the data to calculate the number of observations\n",
    "# With Lazy API, polars will optimize the amount of data read into memory\n",
    "num_obs = df_train.select(pl.len()).collect(streaming=True).item()\n",
    "\n",
    "print(f'Number of observations...: {num_obs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637be85",
   "metadata": {
    "papermill": {
     "duration": 0.007996,
     "end_time": "2024-02-19T00:43:39.766067",
     "exception": false,
     "start_time": "2024-02-19T00:43:39.758071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will drop columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1681d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:43:39.783171Z",
     "iopub.status.busy": "2024-02-19T00:43:39.782827Z",
     "iopub.status.idle": "2024-02-19T00:44:28.649749Z",
     "shell.execute_reply": "2024-02-19T00:44:28.648790Z"
    },
    "papermill": {
     "duration": 48.885799,
     "end_time": "2024-02-19T00:44:28.659350",
     "exception": false,
     "start_time": "2024-02-19T00:43:39.773551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of colums with > 50% missing values: 135\n",
      "Number of columns after dropping: 158\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute the number of missing values in each column and compute the percentages\n",
    "# this op takes time to compute since we need to actually perform\n",
    "# operations on the entire training set\n",
    "missing_perc = df_train.null_count().collect(streaming=True) / num_obs * 100\n",
    "\n",
    "# 2. Get a list of all columns with missing value percentages greater than 50%\n",
    "columns_to_drop = [col for col in missing_perc.columns if missing_perc[col][0] > 50]\n",
    "print(f'Number of colums with > 50% missing values: {len(columns_to_drop)}')\n",
    "\n",
    "# 3. Drop these columns from the training set\n",
    "df_train = df_train.drop(columns_to_drop)\n",
    "schema = df_train.schema\n",
    "print(f'Number of columns after dropping: {len(schema)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d1c9f",
   "metadata": {
    "papermill": {
     "duration": 0.007145,
     "end_time": "2024-02-19T00:44:28.674014",
     "exception": false,
     "start_time": "2024-02-19T00:44:28.666869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are some columns which are categorical. We will drop columns that have only 1 unique value or have way too many unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05629a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:44:28.690493Z",
     "iopub.status.busy": "2024-02-19T00:44:28.689824Z",
     "iopub.status.idle": "2024-02-19T00:46:14.448799Z",
     "shell.execute_reply": "2024-02-19T00:46:14.447495Z"
    },
    "papermill": {
     "duration": 105.776581,
     "end_time": "2024-02-19T00:46:14.457942",
     "exception": false,
     "start_time": "2024-02-19T00:44:28.681361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after dropping: 152\n"
     ]
    }
   ],
   "source": [
    "cat_columns_to_drop = []\n",
    "for column in schema:\n",
    "    if schema[column] == pl.String:\n",
    "        # get frequency \n",
    "        freq = df_train.select(column).unique().select(pl.len()).collect(streaming=True).item()\n",
    "        \n",
    "        if freq == 1 or freq > 20:\n",
    "            cat_columns_to_drop.append(column)\n",
    "\n",
    "# drop these columns\n",
    "df_train = df_train.drop(cat_columns_to_drop)\n",
    "schema = df_train.schema\n",
    "print(f'Number of columns after dropping: {len(schema)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3548b3a",
   "metadata": {
    "papermill": {
     "duration": 0.007335,
     "end_time": "2024-02-19T00:46:14.472819",
     "exception": false,
     "start_time": "2024-02-19T00:46:14.465484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will finally convert the polars LazyFrame to a pandas DataFrame. As far as I know, lightgbm cannot directly process polars dataframes as of yet. Since some of the columns are categorical, we might need additional type processing for these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c584e60c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:46:14.489679Z",
     "iopub.status.busy": "2024-02-19T00:46:14.489006Z",
     "iopub.status.idle": "2024-02-19T00:46:30.466070Z",
     "shell.execute_reply": "2024-02-19T00:46:30.464862Z"
    },
    "papermill": {
     "duration": 15.988884,
     "end_time": "2024-02-19T00:46:30.469015",
     "exception": false,
     "start_time": "2024-02-19T00:46:14.480131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_cols = [column for column in schema if schema[column] == pl.String]\n",
    "# expensive step: since \n",
    "df_train = df_train.collect().to_pandas()\n",
    "df_train[categorical_cols] = df_train[categorical_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6812c61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:46:30.489586Z",
     "iopub.status.busy": "2024-02-19T00:46:30.488619Z",
     "iopub.status.idle": "2024-02-19T00:46:30.632483Z",
     "shell.execute_reply": "2024-02-19T00:46:30.631353Z"
    },
    "papermill": {
     "duration": 0.155896,
     "end_time": "2024-02-19T00:46:30.634769",
     "exception": false,
     "start_time": "2024-02-19T00:46:30.478873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "del data_stores\n",
    "\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d71afb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:46:30.654728Z",
     "iopub.status.busy": "2024-02-19T00:46:30.653587Z",
     "iopub.status.idle": "2024-02-19T00:46:30.689818Z",
     "shell.execute_reply": "2024-02-19T00:46:30.686411Z"
    },
    "papermill": {
     "duration": 0.049268,
     "end_time": "2024-02-19T00:46:30.692817",
     "exception": false,
     "start_time": "2024-02-19T00:46:30.643549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "      <th>MONTH_NUM</th>\n",
       "      <th>WEEKDAY_NUM</th>\n",
       "      <th>days120_123L</th>\n",
       "      <th>days180_256L</th>\n",
       "      <th>days30_165L</th>\n",
       "      <th>days360_512L</th>\n",
       "      <th>days90_310L</th>\n",
       "      <th>...</th>\n",
       "      <th>days_from_decision_lastapplicationdate_877D</th>\n",
       "      <th>days_from_decision_lastapprdate_640D</th>\n",
       "      <th>days_from_decision_max_approvaldate_319D</th>\n",
       "      <th>days_from_decision_max_creationdate_885D</th>\n",
       "      <th>days_from_decision_max_dateactivated_425D</th>\n",
       "      <th>days_from_decision_max_dtlastpmt_581D</th>\n",
       "      <th>days_from_decision_max_dtlastpmtallstes_3545839D</th>\n",
       "      <th>days_from_decision_max_employedfrom_700D</th>\n",
       "      <th>days_from_decision_max_firstnonzeroinstldate_307D</th>\n",
       "      <th>days_from_decision_max_birth_259D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-2102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3245.0</td>\n",
       "      <td>-2071.0</td>\n",
       "      <td>-16105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-9286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-9134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id  WEEK_NUM  target  MONTH_NUM  WEEKDAY_NUM  days120_123L  \\\n",
       "0        0         0       0          1            4           NaN   \n",
       "1        1         0       0          1            4           NaN   \n",
       "2        2         0       0          1            5           NaN   \n",
       "3        3         0       0          1            4           NaN   \n",
       "4        4         0       1          1            5           NaN   \n",
       "\n",
       "   days180_256L  days30_165L  days360_512L  days90_310L  ...  \\\n",
       "0           NaN          NaN           NaN          NaN  ...   \n",
       "1           NaN          NaN           NaN          NaN  ...   \n",
       "2           NaN          NaN           NaN          NaN  ...   \n",
       "3           NaN          NaN           NaN          NaN  ...   \n",
       "4           NaN          NaN           NaN          NaN  ...   \n",
       "\n",
       "  days_from_decision_lastapplicationdate_877D  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                     -2102.0   \n",
       "3                                         4.0   \n",
       "4                                         4.0   \n",
       "\n",
       "  days_from_decision_lastapprdate_640D  \\\n",
       "0                                  NaN   \n",
       "1                                  NaN   \n",
       "2                                  NaN   \n",
       "3                                  NaN   \n",
       "4                                  NaN   \n",
       "\n",
       "  days_from_decision_max_approvaldate_319D  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "\n",
       "   days_from_decision_max_creationdate_885D  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                   -2102.0   \n",
       "3                                       4.0   \n",
       "4                                       4.0   \n",
       "\n",
       "   days_from_decision_max_dateactivated_425D  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "  days_from_decision_max_dtlastpmt_581D  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "3                                   NaN   \n",
       "4                                   NaN   \n",
       "\n",
       "  days_from_decision_max_dtlastpmtallstes_3545839D  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   days_from_decision_max_employedfrom_700D  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                   -3245.0   \n",
       "3                                    -233.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   days_from_decision_max_firstnonzeroinstldate_307D  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                            -2071.0   \n",
       "3                                               35.0   \n",
       "4                                               35.0   \n",
       "\n",
       "   days_from_decision_max_birth_259D  \n",
       "0                             -11874  \n",
       "1                             -22435  \n",
       "2                             -16105  \n",
       "3                              -9286  \n",
       "4                              -9134  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect pandas dataframe\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a136db2",
   "metadata": {
    "papermill": {
     "duration": 0.00883,
     "end_time": "2024-02-19T00:46:30.711855",
     "exception": false,
     "start_time": "2024-02-19T00:46:30.703025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lightgbm Model\n",
    "\n",
    "Credit: https://www.kaggle.com/code/greysky/home-credit-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea618f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:46:30.731773Z",
     "iopub.status.busy": "2024-02-19T00:46:30.730729Z",
     "iopub.status.idle": "2024-02-19T00:57:06.303013Z",
     "shell.execute_reply": "2024-02-19T00:57:06.301648Z"
    },
    "papermill": {
     "duration": 635.584817,
     "end_time": "2024-02-19T00:57:06.305375",
     "exception": false,
     "start_time": "2024-02-19T00:46:30.720558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.798663\n",
      "[200]\tvalid_0's auc: 0.805635\n",
      "[300]\tvalid_0's auc: 0.80797\n",
      "[400]\tvalid_0's auc: 0.808962\n",
      "[500]\tvalid_0's auc: 0.809611\n",
      "[600]\tvalid_0's auc: 0.809999\n",
      "[700]\tvalid_0's auc: 0.810174\n",
      "Early stopping, best iteration is:\n",
      "[713]\tvalid_0's auc: 0.810254\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.799265\n",
      "[200]\tvalid_0's auc: 0.805947\n",
      "[300]\tvalid_0's auc: 0.808563\n",
      "[400]\tvalid_0's auc: 0.809711\n",
      "[500]\tvalid_0's auc: 0.810286\n",
      "[600]\tvalid_0's auc: 0.810745\n",
      "[700]\tvalid_0's auc: 0.811026\n",
      "[800]\tvalid_0's auc: 0.811175\n",
      "[900]\tvalid_0's auc: 0.811409\n",
      "[1000]\tvalid_0's auc: 0.811693\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[981]\tvalid_0's auc: 0.811703\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.802442\n",
      "[200]\tvalid_0's auc: 0.810128\n",
      "[300]\tvalid_0's auc: 0.812948\n",
      "[400]\tvalid_0's auc: 0.813777\n",
      "[500]\tvalid_0's auc: 0.814315\n",
      "[600]\tvalid_0's auc: 0.814746\n",
      "[700]\tvalid_0's auc: 0.815053\n",
      "[800]\tvalid_0's auc: 0.815272\n",
      "[900]\tvalid_0's auc: 0.815563\n",
      "[1000]\tvalid_0's auc: 0.815628\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[976]\tvalid_0's auc: 0.815659\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.804129\n",
      "[200]\tvalid_0's auc: 0.811508\n",
      "[300]\tvalid_0's auc: 0.814002\n",
      "[400]\tvalid_0's auc: 0.814825\n",
      "[500]\tvalid_0's auc: 0.815315\n",
      "[600]\tvalid_0's auc: 0.815868\n",
      "[700]\tvalid_0's auc: 0.81618\n",
      "[800]\tvalid_0's auc: 0.816429\n",
      "[900]\tvalid_0's auc: 0.816711\n",
      "Early stopping, best iteration is:\n",
      "[926]\tvalid_0's auc: 0.816797\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.798825\n",
      "[200]\tvalid_0's auc: 0.807471\n",
      "[300]\tvalid_0's auc: 0.810034\n",
      "[400]\tvalid_0's auc: 0.811081\n",
      "[500]\tvalid_0's auc: 0.811874\n",
      "[600]\tvalid_0's auc: 0.812193\n",
      "[700]\tvalid_0's auc: 0.812448\n",
      "[800]\tvalid_0's auc: 0.812694\n",
      "[900]\tvalid_0's auc: 0.812762\n",
      "[1000]\tvalid_0's auc: 0.812894\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[968]\tvalid_0's auc: 0.812969\n"
     ]
    }
   ],
   "source": [
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"colsample_bytree\": 0.8, \n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"gpu\",\n",
    "}\n",
    "\n",
    "fitted_models = []\n",
    "\n",
    "for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(50)]\n",
    "    )\n",
    "\n",
    "    fitted_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c062174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:57:06.336593Z",
     "iopub.status.busy": "2024-02-19T00:57:06.335587Z",
     "iopub.status.idle": "2024-02-19T00:57:07.142814Z",
     "shell.execute_reply": "2024-02-19T00:57:07.141562Z"
    },
    "papermill": {
     "duration": 0.824888,
     "end_time": "2024-02-19T00:57:07.145210",
     "exception": false,
     "start_time": "2024-02-19T00:57:06.320322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgbm_fitted.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save fitted models\n",
    "import joblib\n",
    "joblib.dump(fitted_models, 'lgbm_fitted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c5f47",
   "metadata": {
    "papermill": {
     "duration": 0.014611,
     "end_time": "2024-02-19T00:57:07.173937",
     "exception": false,
     "start_time": "2024-02-19T00:57:07.159326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07b2d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:57:07.204564Z",
     "iopub.status.busy": "2024-02-19T00:57:07.203621Z",
     "iopub.status.idle": "2024-02-19T00:57:07.430174Z",
     "shell.execute_reply": "2024-02-19T00:57:07.429165Z"
    },
    "papermill": {
     "duration": 0.244956,
     "end_time": "2024-02-19T00:57:07.433016",
     "exception": false,
     "start_time": "2024-02-19T00:57:07.188060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credit: https://www.kaggle.com/code/greysky/home-credit-baseline\n",
    "data_stores = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"other_dfs\": [\n",
    "        # depth 0\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "        # depth 1\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "        # depth 2\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# process test set in appropriate formate\n",
    "df_test = (\n",
    "    merge_and_basic_fe(**data_stores)\n",
    "    .drop(columns_to_drop) # drop columns with too many missing values\n",
    "    .drop(cat_columns_to_drop) # drop columns with too many categorical values\n",
    "    .collect().to_pandas()\n",
    "    .set_index('case_id')\n",
    "    .drop(columns=['WEEK_NUM'])\n",
    ")\n",
    "\n",
    "df_test[categorical_cols] = df_test[categorical_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f889fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T00:57:07.466903Z",
     "iopub.status.busy": "2024-02-19T00:57:07.466218Z",
     "iopub.status.idle": "2024-02-19T00:57:07.548508Z",
     "shell.execute_reply": "2024-02-19T00:57:07.547379Z"
    },
    "papermill": {
     "duration": 0.101666,
     "end_time": "2024-02-19T00:57:07.551393",
     "exception": false,
     "start_time": "2024-02-19T00:57:07.449727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate predictions (avgd probabilities)\n",
    "prob_preds = np.column_stack([\n",
    "    model.predict_proba(df_test)[:, 1] for model in fitted_models\n",
    "]).mean(axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'case_id': df_test.index.tolist(),\n",
    "    'score': prob_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732224ea",
   "metadata": {
    "papermill": {
     "duration": 0.014297,
     "end_time": "2024-02-19T00:57:07.580433",
     "exception": false,
     "start_time": "2024-02-19T00:57:07.566136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7602123,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 824.848133,
   "end_time": "2024-02-19T00:57:08.719698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-19T00:43:23.871565",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
