{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/syerramilli/ps3e22-eda-catboost-baseline?scriptVersionId=143927461\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Introduction\n\nThe objective of this classification task is to predict the health outcomes of horses based on their historical medical data. There are three potential outcomes: \"lived,\" \"died,\" and \"euthanized.\" Within this notebook, we conduct exploratory data analysis (EDA), clean and preprocess the dataset, and subsequently train a pair of baseline CatBoost classification models â€” without any hyperparameter tuning. We've opted for the CatBoost model as a baseline as it typically has reasonable performance straight out of the box. Furthermore, we delve into assessing feature importance through a SHAP analysis. Eventually, we leverage these SHAP values to identify a subset of features, which we then employ to train a second baseline model. Interestingly, this second baseline model exhibits a slight improvement in performance compared to the first one, as assessed by cross-validation micro F1 scores. \n\nA couple of directions to improve the performance of the baselines:\n1. Careful feature selection and feature engineering\n2. Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom scipy.stats import chi2_contingency # for association between different categorical variables\n\nfrom numbers import Number \nfrom pathlib import Path\nfrom typing import Optional, Dict\n\nplt.style.use('ggplot')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-23T03:23:18.16915Z","iopub.execute_input":"2023-09-23T03:23:18.169607Z","iopub.status.idle":"2023-09-23T03:23:19.323635Z","shell.execute_reply.started":"2023-09-23T03:23:18.16957Z","shell.execute_reply":"2023-09-23T03:23:19.322546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data","metadata":{}},{"cell_type":"code","source":"path = Path('/kaggle/input/playground-series-s3e22')\ntrain = pd.read_csv(path/'train.csv',index_col=['id'])\ntest = pd.read_csv(path/'test.csv',index_col=['id'])\n\ndel train['hospital_number']\ndel test['hospital_number']\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.327044Z","iopub.execute_input":"2023-09-23T03:23:19.327713Z","iopub.status.idle":"2023-09-23T03:23:19.437813Z","shell.execute_reply.started":"2023-09-23T03:23:19.327679Z","shell.execute_reply":"2023-09-23T03:23:19.436511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of rows in training set: {train.shape[0]}')\nprint(f'Number of rows in test set: {test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.439221Z","iopub.execute_input":"2023-09-23T03:23:19.439619Z","iopub.status.idle":"2023-09-23T03:23:19.444924Z","shell.execute_reply.started":"2023-09-23T03:23:19.439589Z","shell.execute_reply":"2023-09-23T03:23:19.443867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a quick snapshot of the training data. From this snapshot, we can see the following\n1. There are 26 potential feature columns. \n2. There are several numerical features (indicated by dtypes `float64` and `int64` dtypes). None of them have missing entries.\n3. There are several features with dtype - `object`. These are likely categorical valued, although some of them like `age` may be processed as numerical. \n4. Among the columns with the `object` dtype, there are a few entries with missing values.","metadata":{}},{"cell_type":"code","source":"# quick snapshot\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.447555Z","iopub.execute_input":"2023-09-23T03:23:19.448111Z","iopub.status.idle":"2023-09-23T03:23:19.48094Z","shell.execute_reply.started":"2023-09-23T03:23:19.448082Z","shell.execute_reply":"2023-09-23T03:23:19.479694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target\n\nThe goal of this task to predict `outcome` - whether the horse survived or not - based on past medical conditions. There are three classes: lived, died, and euthanized. In the cell below, we show the bar plot of the counts for each class. ","metadata":{}},{"cell_type":"code","source":"train['outcome'].value_counts().plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.48264Z","iopub.execute_input":"2023-09-23T03:23:19.483048Z","iopub.status.idle":"2023-09-23T03:23:19.796652Z","shell.execute_reply.started":"2023-09-23T03:23:19.483013Z","shell.execute_reply":"2023-09-23T03:23:19.795372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lesions \n\nFrom the description in the [original dataset](https://www.kaggle.com/datasets/yasserh/horse-survival-dataset), the columns `lesion_1`, `lesion_2` and `lesion_3` encode the site, type, subtype, and specific code. ","metadata":{}},{"cell_type":"code","source":"for i in range(1,4):\n    column = f'lesion_{i}'\n    print(f\"Number of unique values in {column}: {train[column].nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.797877Z","iopub.execute_input":"2023-09-23T03:23:19.798791Z","iopub.status.idle":"2023-09-23T03:23:19.807217Z","shell.execute_reply.started":"2023-09-23T03:23:19.798757Z","shell.execute_reply":"2023-09-23T03:23:19.805992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It turns out almost all the entires of `lesion_2` and `lesion_3` are 0. Therefore, we will drop them in the remaining analysis.","metadata":{}},{"cell_type":"code","source":"for i in range(2,4):\n    print(train[f'lesion_{i}'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.80879Z","iopub.execute_input":"2023-09-23T03:23:19.809217Z","iopub.status.idle":"2023-09-23T03:23:19.820157Z","shell.execute_reply.started":"2023-09-23T03:23:19.809185Z","shell.execute_reply":"2023-09-23T03:23:19.81895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping lesion_2 and lesion_3\ntrain = train.drop(['lesion_2','lesion_3'],axis=1)\ntest = test.drop(['lesion_2','lesion_3'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.821992Z","iopub.execute_input":"2023-09-23T03:23:19.823067Z","iopub.status.idle":"2023-09-23T03:23:19.832845Z","shell.execute_reply.started":"2023-09-23T03:23:19.82301Z","shell.execute_reply":"2023-09-23T03:23:19.831503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following, we decode the lesion sites and types\n\n### Lesion site\n\nThere are 10 different lesion sites and a case where there are no lesions.","metadata":{}},{"cell_type":"code","source":"def map_lesion_site(value:str) -> str:\n    if value[:2] == \"11\" and len(value) == 5:\n        return \"all_intestinal\"\n    elif value[0] == \"1\":\n        return \"gastric\"\n    elif value[0] == \"2\":\n        return \"sm_intestine\"\n    elif value[0] == \"3\":\n        return \"lg_colon\"\n    elif value[0] == \"4\":\n        return \"lg_colon_and_cecum\"\n    elif value[0] == \"5\":\n        return \"cecum\"\n    elif value[0] == \"6\":\n        return \"transverse_colon\"\n    elif value[0] == \"7\":\n        return \"retum_colon\"\n    elif value[0] == \"8\":\n        return \"uterus\"\n    elif value[0] == \"9\":\n        return \"bladder\"\n    elif value[0] == \"0\":\n        return \"none\"\n    else:\n        return \"ERROR\"\n    \ntrain['lesion_site'] = train['lesion_1'].astype(str).apply(map_lesion_site)\ntest['lesion_site'] = test['lesion_1'].astype(str).apply(map_lesion_site)\n\nfig,ax = plt.subplots(1, 1, figsize=(6,4))\ntrain['lesion_site'].value_counts().plot(kind='bar', ax=ax)\n_ = ax.tick_params(axis='x', rotation=60)\n# to be used later\nsite_order = [text.get_text() for text in ax.get_xticklabels()]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:19.836854Z","iopub.execute_input":"2023-09-23T03:23:19.837355Z","iopub.status.idle":"2023-09-23T03:23:20.194849Z","shell.execute_reply.started":"2023-09-23T03:23:19.837321Z","shell.execute_reply":"2023-09-23T03:23:20.193689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the ","metadata":{}},{"cell_type":"code","source":"ax = (\n    train\n    .groupby('lesion_site')['outcome']\n    .value_counts(normalize=True)\n    .mul(100)\n    .rename('Percentage')\n    .reset_index()\n    .pipe(\n        (sns.catplot,'data'), y='lesion_site',x='Percentage',hue='outcome',\n        order= site_order,\n        hue_order=['died', 'euthanized', 'lived'],\n        kind='bar', height=5, aspect=7/5\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:20.199605Z","iopub.execute_input":"2023-09-23T03:23:20.199956Z","iopub.status.idle":"2023-09-23T03:23:21.073505Z","shell.execute_reply.started":"2023-09-23T03:23:20.199927Z","shell.execute_reply":"2023-09-23T03:23:21.072145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lesion type\n","metadata":{}},{"cell_type":"code","source":"def map_lesion_type(value:str) -> str:\n    if value == '0':\n        return \"none\"\n    \n    value2 = value[2] if len(value)==5 else value[1]\n    \n    if value2 == '1':\n        return \"simple\"\n    elif value2 == '2':\n        return 'strangulation'\n    elif value2 == '3':\n        return 'inflammation'\n    elif value2 == '4':\n        return 'other'\n    \n    return 'ERROR'\n\ntrain['lesion_type'] = train['lesion_1'].astype(str).apply(map_lesion_type)\ntest['lesion_type'] = test['lesion_1'].astype(str).apply(map_lesion_type)\n\nfig,ax = plt.subplots(1, 1, figsize=(6,4))\ntrain['lesion_type'].value_counts().plot(kind='bar', ax=ax)\n_ = ax.tick_params(axis='x', rotation=60)\n# to be used later\ntype_order = [text.get_text() for text in ax.get_xticklabels()]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:21.075316Z","iopub.execute_input":"2023-09-23T03:23:21.075665Z","iopub.status.idle":"2023-09-23T03:23:21.375715Z","shell.execute_reply.started":"2023-09-23T03:23:21.075636Z","shell.execute_reply":"2023-09-23T03:23:21.374753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = (\n    train\n    .groupby('lesion_type')['outcome']\n    .value_counts(normalize=True)\n    .mul(100)\n    .rename('Percentage')\n    .reset_index()\n    .pipe(\n        (sns.catplot,'data'), y='lesion_type',x='Percentage',hue='outcome',\n        order= type_order,\n        hue_order=['died', 'euthanized', 'lived'],\n        kind='bar', height=4, aspect=6/4\n    )\n)\n#_ = ax.tick_params(axis='x', rotation=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:21.377213Z","iopub.execute_input":"2023-09-23T03:23:21.377938Z","iopub.status.idle":"2023-09-23T03:23:21.993602Z","shell.execute_reply.started":"2023-09-23T03:23:21.377893Z","shell.execute_reply":"2023-09-23T03:23:21.992399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO**: Decode the lesion subtype and code. ","metadata":{}},{"cell_type":"code","source":"# delete lesion_1\ndel train['lesion_1']\ndel test['lesion_1']","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:21.994984Z","iopub.execute_input":"2023-09-23T03:23:21.995805Z","iopub.status.idle":"2023-09-23T03:23:22.001685Z","shell.execute_reply.started":"2023-09-23T03:23:21.995774Z","shell.execute_reply":"2023-09-23T03:23:22.000627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Features","metadata":{}},{"cell_type":"code","source":"numerical_cols= train.select_dtypes(include=['number']).columns.tolist()\nprint(f'Number of numerical columns: {len(numerical_cols)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:22.003077Z","iopub.execute_input":"2023-09-23T03:23:22.004015Z","iopub.status.idle":"2023-09-23T03:23:22.015501Z","shell.execute_reply.started":"2023-09-23T03:23:22.003984Z","shell.execute_reply":"2023-09-23T03:23:22.014301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the histograms of the 10 numerical features in the cell below.  The features `respiratory_rate`, `total_protein`, and `abdomo_protein`  have positive skew.","metadata":{}},{"cell_type":"code","source":"n_rows = 2\nn_cols = 4\nfig,axs = plt.subplots(n_rows,n_cols,figsize=(4*n_cols,3*n_rows))\nfor i in range(n_rows):\n    for j in range(n_cols):\n        col_index = n_cols*i+j\n        if col_index == 7:\n            break\n        _ = sns.histplot(data=train,x=numerical_cols[col_index], ax=axs[i,j],bins=20)\n        \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:22.017002Z","iopub.execute_input":"2023-09-23T03:23:22.017336Z","iopub.status.idle":"2023-09-23T03:23:24.273759Z","shell.execute_reply.started":"2023-09-23T03:23:22.017308Z","shell.execute_reply":"2023-09-23T03:23:24.272649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To confirm our observations about the skew in the distributions in some of the features, we compute the skewness statistic for the remaining 8 numerical features.","metadata":{}},{"cell_type":"code","source":"skewness = train[numerical_cols].skew().sort_values(ascending=False)\nskewness","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:24.274827Z","iopub.execute_input":"2023-09-23T03:23:24.275152Z","iopub.status.idle":"2023-09-23T03:23:24.289128Z","shell.execute_reply.started":"2023-09-23T03:23:24.275124Z","shell.execute_reply":"2023-09-23T03:23:24.288314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now plot the boxplots of the features grouped by the different classes. For the 4 features with significant positive skew, we apply a log transform before generating the boxplot. Some observations:\n\n1. The horses that lived generally had a lower `pulse`  than the other groups.\n2. Horses that were euthanized had orders of magnitude higher `total_protein` than the other two groups, although there are quite a few outliers in the other groups.","metadata":{}},{"cell_type":"code","source":"n_rows = 2\nn_cols = 4\nfig,axs = plt.subplots(n_rows,n_cols,figsize=(3*n_cols,3*n_rows))\nfor i in range(n_rows):\n    for j in range(n_cols):\n        col_index = n_cols*i+j\n        if col_index == 7:\n            break\n        \n        \n        column = numerical_cols[col_index]\n        \n        if skewness.loc[column] > 1:\n            _ = sns.boxplot(y=np.log(train[column]),x=train['outcome'],ax=axs[i,j])\n            _ = axs[i,j].set_ylabel(f'log({column})')\n        else:\n            _ = sns.boxplot(data=train,y=column,x='outcome', ax=axs[i,j])\n        \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:24.29037Z","iopub.execute_input":"2023-09-23T03:23:24.290875Z","iopub.status.idle":"2023-09-23T03:23:26.210881Z","shell.execute_reply.started":"2023-09-23T03:23:24.290845Z","shell.execute_reply":"2023-09-23T03:23:26.209583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the histogram and density plots of the features grouped by outcome. We applied a log transform to the four features with significant positive skew before generating the histograms and estimating the densities. Here are some key observations:\n\n1. When examining the conditional histogram for the `pulse` feature in the context of the \"lived\" outcomes, a distinct mode is evident. This suggests that the `pulse` feature can potentially serve as a predictor for the outcome of \"lived.\"\n2. However, there is no such clear pattern for predicting the other two outcome classes. The modes of these classes often coincide with at least one other class. For instance, consider the `log_protein` feature. The mode for the \"died\" outcomes aligns with the higher density mode for the \"lived\" outcomes, and similarly, a mode for the \"euthanized\" outcomes coincides with a mode for the \"lived\" outcomes.","metadata":{}},{"cell_type":"code","source":"n_rows = 2\nn_cols = 4\nfig,axs = plt.subplots(n_rows,n_cols,figsize=(5*n_cols,3*n_rows))\nfor i in range(n_rows):\n    for j in range(n_cols):\n        col_index = n_cols*i+j\n        if col_index == 7:\n            break\n        \n        \n        column = numerical_cols[col_index]\n        \n        if skewness.loc[column] > 1:\n            _ = sns.histplot(x=np.log(train[column]), hue=train['outcome'],ax=axs[i,j], bins=20, stat='density', kde=True)\n            _ = axs[i,j].set_xlabel(f'log({column})')\n        else:\n            _ = sns.histplot(data=train,x=column,hue='outcome', ax=axs[i,j], bins=20, stat='density', kde=True)\n            \n        if col_index > 0:\n            _ = axs[i,j].get_legend().remove()\n        \nfig.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T03:23:26.214325Z","iopub.execute_input":"2023-09-23T03:23:26.215294Z","iopub.status.idle":"2023-09-23T03:23:29.956783Z","shell.execute_reply.started":"2023-09-23T03:23:26.215243Z","shell.execute_reply":"2023-09-23T03:23:29.955479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we plot the correlation heatmap, where we compute the Spearman rank correlation. There are no red flags here.","metadata":{}},{"cell_type":"code","source":"corr_matrix = train[numerical_cols].corr(method='spearman')\nmask =np.triu(np.ones_like(corr_matrix, dtype=bool))\nfig,ax = plt.subplots(1,1,figsize=(5,4),dpi=150)\n_ = sns.heatmap(corr_matrix,annot=True,fmt='.2f',mask=mask,ax=ax)\n_ = ax.set_facecolor('w')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:23:29.958557Z","iopub.execute_input":"2023-09-23T03:23:29.959768Z","iopub.status.idle":"2023-09-23T03:23:30.492476Z","shell.execute_reply.started":"2023-09-23T03:23:29.959722Z","shell.execute_reply":"2023-09-23T03:23:30.491572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical feature engineering\n\nFrom the conditional density plots, no one feature had distinct modes for the \"died\" and \"euthanized\" classes. However, through some empirical experimentation, we have identified a specific feature characterized by the logarithm of the ratio between the square of the pulse and the total_protein that exhibits a notable and distinguishable mode specifically within the \"died\" outcome category.","metadata":{}},{"cell_type":"code","source":"train['log_pulseSq_total_protein'] = -np.log(train['total_protein']) + 2*np.log(train['pulse'])\ntest['log_pulseSq_total_protein'] = -np.log(test['total_protein']) + 2*np.log(test['pulse'])\n\n\n_ = sns.histplot(data=train, x='log_pulseSq_total_protein',hue='outcome', kde=True, stat='density')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:31:10.247965Z","iopub.execute_input":"2023-09-23T03:31:10.248433Z","iopub.status.idle":"2023-09-23T03:31:10.821332Z","shell.execute_reply.started":"2023-09-23T03:31:10.248397Z","shell.execute_reply":"2023-09-23T03:31:10.820199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"rem_columns = train.drop('outcome',axis=1).select_dtypes(include=['object']).columns.tolist()\nprint(f'Number of columns with dtype object: {len(rem_columns)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:47:58.639307Z","iopub.execute_input":"2023-09-23T03:47:58.63975Z","iopub.status.idle":"2023-09-23T03:47:58.650602Z","shell.execute_reply.started":"2023-09-23T03:47:58.639713Z","shell.execute_reply":"2023-09-23T03:47:58.649319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will exlcude any column where the mode (aka most common value) occurs in more than 85% of the entries.","metadata":{}},{"cell_type":"code","source":"def get_mode_fraction(x:pd.Series) -> float:\n    cts = x.value_counts(sort=True, ascending=False)\n    return cts.iloc[0]/x.shape[0]\n\nfor i, column in enumerate(rem_columns):\n    mode_frac = get_mode_fraction(train[column])\n    if mode_frac > 0.85:\n        # drop the feature if >85% of the observations \n        # belong to the mode\n        print(f'Dropping {column} with the mode having {mode_frac*100:.2f}% observations')\n        \n        del train[column]\n        del test[column]\n        \n        rem_columns.pop(i)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:00.051737Z","iopub.execute_input":"2023-09-23T03:48:00.052457Z","iopub.status.idle":"2023-09-23T03:48:00.072512Z","shell.execute_reply.started":"2023-09-23T03:48:00.052411Z","shell.execute_reply":"2023-09-23T03:48:00.071345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now run chi-squared contigency tests to test the significance of the relationships of each categorical feature with the response. It appears that all 16 features have significant relationship with `outcome`.","metadata":{}},{"cell_type":"code","source":"def contingency_test(input_col:str, significance_level:float=0.01) -> bool:\n    stat,pval,_,_ = chi2_contingency(pd.crosstab(train[input_col], train['outcome']))\n    \n    return abs(stat), pval < significance_level\n\nchi2_tests_df = pd.DataFrame(\n    [contingency_test(column) for column in rem_columns],\n    index=rem_columns,\n    columns=['abs_stat', 'is_significant']\n).sort_values(by=['is_significant', 'abs_stat'], ascending=False)\n\nprint(f'Number of categorical features with signficant relationship with outcome: {chi2_tests_df[\"is_significant\"].sum()}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:01.521784Z","iopub.execute_input":"2023-09-23T03:48:01.522203Z","iopub.status.idle":"2023-09-23T03:48:01.679482Z","shell.execute_reply.started":"2023-09-23T03:48:01.52217Z","shell.execute_reply":"2023-09-23T03:48:01.678317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we generate a bar plots of the counts of the categories for each of these features, *except the two lesion features decoded earlier*. Within each category, we separate the counts by class. Some preliminary observations:\n\n1. For all the features, the class counts seem to differ between atleast two categories, suggesting some relationship.\n2. Some of the features like `temp_of_extremities`can be encoded as ordinal integers. This can help reduce the dimensionaloity.\n3. For a lot of features, some of the categories have very few observations. We might need to merge these categories to learn something useful. ","metadata":{}},{"cell_type":"code","source":"n_rows = 5\nn_cols = 3\nfig,axs = plt.subplots(n_rows,n_cols,figsize=(5*n_cols,4*n_rows))\nfor i in range(n_rows):\n    for j in range(n_cols):\n        column = rem_columns[n_cols*i+j]\n        _ = sns.countplot(data=train,x=column,hue='outcome', ax=axs[i,j])\n        _ = axs[i,j].tick_params(axis='x', rotation=30)\n        \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:03.028246Z","iopub.execute_input":"2023-09-23T03:48:03.028688Z","iopub.status.idle":"2023-09-23T03:48:07.627238Z","shell.execute_reply.started":"2023-09-23T03:48:03.028658Z","shell.execute_reply":"2023-09-23T03:48:07.62611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `preprocess_categorical` function performs the following operations:\n1. Replaces erroneous categories in a couple of features with pd.NA\n2. Merges categories with very few observations onto a different category\n3. Ordinal encodes columns that are either binary valued or have some inherent order\n4. Converts the `dtype` of the remaining columns to `pd.Categorical`. (while this step isn't needed for catboost, it will be useful for models such as lightgbm that can natively handle categorical features). \n\n**TODO**: Merge categories of the `lesion_site` feature so that the model can learn something useful for categories with very few observations.","metadata":{}},{"cell_type":"code","source":"categorical_columns = [\n    'mucous_membrane', 'abdomen','rectal_exam_feces', \n    'lesion_site', 'lesion_type' \n]\n\ndef preprocess_categorical(df:pd.DataFrame) -> None:\n    # cleaning some of the categorical features\n    df['peristalsis'] = df['peristalsis'].replace('distend_small',pd.NA)\n    df['rectal_exam_feces'] = df['rectal_exam_feces'].replace('serosanguious',pd.NA)\n    \n    # merging some of the categories\n    df['capillary_refill_time'] = df['capillary_refill_time'].replace('3','more_3_sec')\n    df['pain'] = df['pain'].replace('slight','alert')\n    \n    \n    # encoding some of the catgeorical level \n    ordinal_and_binary_dict = {\n        'surgery': ['no','yes'], \n        'temp_of_extremities': ['cold','cool', 'normal', 'warm'], \n        'peripheral_pulse': ['absent','reduced', 'normal','increased'], \n        'pain':['alert', 'depressed', 'mild_pain', 'moderate', 'severe_pain', 'extreme_pain'],\n        'capillary_refill_time': ['less_3_sec', 'more_3_sec'], \n        'peristalsis': ['absent', 'hypomotile', 'normal', 'hypermotile'], \n        'abdominal_distention': ['none', 'slight', 'moderate', 'severe'], \n        'nasogastric_tube': ['none', 'slight', 'significant'], \n        'nasogastric_reflux': ['none','slight','less_1_liter', 'more_1_liter'], \n        'abdomo_appearance': ['serosanguious', 'cloudy', 'clear'], \n        'surgical_lesion': ['no', 'yes'], \n        'cp_data': ['no', 'yes']\n    }\n    \n    for column, levels in ordinal_and_binary_dict.items():\n        df[column] = df[column].replace({\n            level:i for i,level in enumerate(levels)\n        })\n        \n    for column in categorical_columns:\n        # useful for other featur\n        df[column] = df[column].astype('category')\n        \n\n# modify columns in place\npreprocess_categorical(train)\npreprocess_categorical(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:07.629645Z","iopub.execute_input":"2023-09-23T03:48:07.629988Z","iopub.status.idle":"2023-09-23T03:48:07.693843Z","shell.execute_reply.started":"2023-09-23T03:48:07.629959Z","shell.execute_reply":"2023-09-23T03:48:07.692795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing values\n\nIn the quick snapshot earlier, we found that some of the features (encoded as `object`) have some values missing. In the cell below, I compute the fraction of missing values in each column. (Note: Columns with no missing values are excluded).","metadata":{}},{"cell_type":"code","source":"def filter_greater_than(series:pd.Series,threshold:Number) -> pd.Series:\n    '''\n    Returns series elements greater than threshold. This funtion can be\n    used with the .pipe methods\n    '''\n    return series[series>threshold]\n\ndef get_perc_missing(df:pd.DataFrame) -> pd.Series:\n    return (\n        (df.isnull().sum()/df.shape[0]*100)\n        .sort_values(ascending=False)\n        .pipe(filter_greater_than,threshold=0)\n        .round(3)\n    )\n\nperc_missing = get_perc_missing(train)\nperc_missing","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:07.695233Z","iopub.execute_input":"2023-09-23T03:48:07.695587Z","iopub.status.idle":"2023-09-23T03:48:07.712836Z","shell.execute_reply.started":"2023-09-23T03:48:07.695559Z","shell.execute_reply":"2023-09-23T03:48:07.711549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same columns have missing entries in the test set too, and the percentage of missing values in the test set is roughly the same. So, we will need a concrete imputation strategy.","metadata":{}},{"cell_type":"code","source":"perc_missing_test = get_perc_missing(test)\nassert perc_missing.shape[0] == perc_missing_test.shape[0]\nperc_missing_test","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:09.167049Z","iopub.execute_input":"2023-09-23T03:48:09.167519Z","iopub.status.idle":"2023-09-23T03:48:09.180924Z","shell.execute_reply.started":"2023-09-23T03:48:09.167465Z","shell.execute_reply":"2023-09-23T03:48:09.180091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For `abdomen` and `rectal_exam_feces`, we add a new category called `\"missing\"` for the missing entries.","metadata":{}},{"cell_type":"code","source":"for column in ['abdomen','rectal_exam_feces']:\n    train[column] = train[column].astype('object').fillna('missing').astype('category')\n    test[column] = test[column].astype('object').fillna('missing').astype('category')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:10.889595Z","iopub.execute_input":"2023-09-23T03:48:10.890016Z","iopub.status.idle":"2023-09-23T03:48:10.902483Z","shell.execute_reply.started":"2023-09-23T03:48:10.889982Z","shell.execute_reply":"2023-09-23T03:48:10.901418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the remaining columns, we impute the missing value with the mode. \n\nTODO: Use a more systematic imputation strategy.","metadata":{}},{"cell_type":"code","source":"for column in perc_missing.iloc[2:].index:\n    mode_col = train[column].mode().iloc[0]\n    train[column] = train[column].fillna(mode_col)\n    test[column] = test[column].fillna(mode_col)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:12.725558Z","iopub.execute_input":"2023-09-23T03:48:12.725957Z","iopub.status.idle":"2023-09-23T03:48:12.742718Z","shell.execute_reply.started":"2023-09-23T03:48:12.725927Z","shell.execute_reply":"2023-09-23T03:48:12.741747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost model\n\nThe function `fit_model` in the cell below, trains a catboost classification model that uses bagging for the individual trees. The function also allows the specification of hyperparameters as a dictionary through the `config` argument. If `config` is not specified, default values for the hyperparameters are used.","metadata":{}},{"cell_type":"code","source":"def fit_model(\n    X:pd.DataFrame,\n    y:np.ndarray,\n    config:Optional[Dict]=None,\n    n_jobs:int=1,\n    verbose:int=0,\n    random_seed:int=100,\n) -> CatBoostClassifier:\n    '''\n    Train a catboost classifier\n    '''\n    model = CatBoostClassifier(\n        iterations = 500,\n        thread_count = n_jobs,\n        bootstrap_type = 'Bernoulli',\n        subsample = 0.8,\n        random_seed = random_seed,\n        verbose = verbose\n    )\n    \n    if config:\n        # if config is supplied, set the model hyperparameters\n        model.set_params(**config)\n        \n    cat_features = [\n        column for column in X.columns if X[column].dtype == 'category'\n    ]\n        \n    return model.fit(X, y, cat_features= cat_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:16.866034Z","iopub.execute_input":"2023-09-23T03:48:16.866483Z","iopub.status.idle":"2023-09-23T03:48:16.875478Z","shell.execute_reply.started":"2023-09-23T03:48:16.86645Z","shell.execute_reply":"2023-09-23T03:48:16.87414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop('outcome',axis=1)\nle = LabelEncoder()\ny = le.fit_transform(train['outcome'].values)\n\nmodel = fit_model(X, y, n_jobs=4, verbose=50, random_seed=100)\nmodel.save_model('baseline.cbm',format='cbm')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:18.836425Z","iopub.execute_input":"2023-09-23T03:48:18.836838Z","iopub.status.idle":"2023-09-23T03:48:23.049998Z","shell.execute_reply.started":"2023-09-23T03:48:18.836786Z","shell.execute_reply":"2023-09-23T03:48:23.048854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation\n\nTo provide a numerical measure for the baseline, we will use the f1score estimate from 4 replicates of 10-fold stratified cross-validation. We use replicated CV here since the size of the training set is small.","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom tqdm import tqdm\ndef fit_and_test_fold(X, y, train_index,test_index) -> float:\n    X_train = X.iloc[train_index,:];X_test = X.iloc[test_index,:]\n    y_train = y[train_index]; y_test = y[test_index]\n    \n    # fit model on training data\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        model = fit_model(X_train, y_train, n_jobs=4)\n    \n    # generate predictions on test data\n    test_pred = model.predict(X_test)\n    \n    return f1_score(y_test, test_pred, average='micro')\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=4, random_state=1)\ncv_f1_scores = [None]*40\nfor i, (train_index, test_index) in tqdm(enumerate(cv.split(X,y))):\n    cv_f1_scores[i] = fit_and_test_fold(X, y, train_index, test_index)\n\ncv_f1 = np.mean(cv_f1_scores)\nprint(f'CV F1 for baseline model: {cv_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:49:04.899145Z","iopub.execute_input":"2023-09-23T03:49:04.900242Z","iopub.status.idle":"2023-09-23T03:51:46.548871Z","shell.execute_reply.started":"2023-09-23T03:49:04.900194Z","shell.execute_reply":"2023-09-23T03:51:46.547725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CV F1 for each replicate of 10-fold CV\n# Clearly, there is some variability across replicates\nnp.array(cv_f1_scores).reshape(-1,10).mean(-1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:51:46.551223Z","iopub.execute_input":"2023-09-23T03:51:46.551647Z","iopub.status.idle":"2023-09-23T03:51:46.559999Z","shell.execute_reply.started":"2023-09-23T03:51:46.551609Z","shell.execute_reply":"2023-09-23T03:51:46.558883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importances\n\n\n\nWe now compute the gain based feature importance measures from the catboost model.\n\n**Notes**:\n1. Feature importance measures from tree based models can be misleading.\n2. In catboost, the default feature importance measure is based on the total gain from splits involving the feature.","metadata":{}},{"cell_type":"code","source":"# gain based feature importances - not necessarily the most reliable\nfeat_imp = pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True)\nfeat_imp.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2023-09-22T13:54:34.76359Z","iopub.execute_input":"2023-09-22T13:54:34.764404Z","iopub.status.idle":"2023-09-22T13:54:35.260451Z","shell.execute_reply.started":"2023-09-22T13:54:34.764359Z","shell.execute_reply":"2023-09-22T13:54:35.259585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importances through SHAP\n\nThe default feature importances computed by catboost (or any tree based model) can be misleading. Here, we will use SHAP measures to check the importance of each feature.\n\nSHAP values represent the impact of each feature on the model's output for a specific instance. In multiclass classification, we will have a **separate** set of SHAP values for each class. These values tell us how each feature contributes to each class prediction, i.e., distinguishing the specific class from the rest.","metadata":{}},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:30.756414Z","iopub.execute_input":"2023-09-23T03:48:30.756937Z","iopub.status.idle":"2023-09-23T03:48:37.343015Z","shell.execute_reply.started":"2023-09-23T03:48:30.756897Z","shell.execute_reply":"2023-09-23T03:48:37.341779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the SHAP summary plot, we plot a horizontal bar plot of the absolute SHAP value for each feature averaged across the observations.\nFeatures with longer bars have a higher influence on the model's output for the specific class. Since we have 3 classes here, we will see 3 stacked bars for each feature. The features are ordered according to the cumulative length of the 3 bars. \n\nLet's look at the top 3 features from the plot below. \n\n1. `lesion_type` is very important for predicting \"lived\" outcomes,  and moderately important for predicting \"died\" outcomes and \"euthanized\" outcomes.\n2. `total_protein` is very important for predicting \"died\" and \"euthanized\" outcomes, but not important for predicting \"lived\" outcomes.\n3. `pain` is important for predicting \"lived\" and \"died\" outcomes, but not important for predicting \"euthanized\" outcomes.","metadata":{}},{"cell_type":"code","source":"# Average of SHAP value magnitudes across the dataset\nshap.summary_plot(\n    shap_values, X, plot_type=\"bar\",\n    class_names = le.classes_,\n    plot_size = (10,6)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:37.345821Z","iopub.execute_input":"2023-09-23T03:48:37.346556Z","iopub.status.idle":"2023-09-23T03:48:38.33874Z","shell.execute_reply.started":"2023-09-23T03:48:37.346515Z","shell.execute_reply":"2023-09-23T03:48:38.337445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we show the SHAP values separately by class. We only show the top 10 features.","metadata":{}},{"cell_type":"code","source":"avg_shap_class = [\n    pd.Series(\n        np.abs(shap_values[i]).mean(0),\n        index = X.columns.tolist()\n    ).sort_values(ascending=True) for i in range(3)\n]\n\nfig, axs = plt.subplots(1, 3, figsize=(15,4), dpi=150)\nfor i in range(3):\n    _ = avg_shap_class[i].iloc[-10:].plot(kind='barh', ax=axs[i])\n    _ = axs[i].set_title(f'Class: {le.classes_[i]}')\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:45.332217Z","iopub.execute_input":"2023-09-23T03:48:45.332716Z","iopub.status.idle":"2023-09-23T03:48:46.399814Z","shell.execute_reply.started":"2023-09-23T03:48:45.332682Z","shell.execute_reply":"2023-09-23T03:48:46.39867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dependence plots\n\nIn the cell below, we plot the SHAP dependence plots for the top 3 features for each class. ","metadata":{}},{"cell_type":"code","source":"n_features = 3\nfor i in range(3):\n    fig, axs = plt.subplots(1, n_features, figsize=(5*n_features,4), dpi=100)\n    \n    features = avg_shap_class[i].iloc[-n_features:].iloc[::-1].index.tolist()\n    \n    for j, feature in enumerate(features):\n        _ = shap.dependence_plot(\n            feature, shap_values[i], X, \n            interaction_index= None, alpha=0.7,\n            ax = axs[j], show=False\n        )\n        if X[feature].dtype == 'category':\n            _ = axs[j].tick_params(axis='x', rotation=60)\n        \n    fig.suptitle(f'SHAP dependence plots for class={le.classes_[i]}')\n    fig.tight_layout(rect=[0,0,1,0.99])\n    fig.savefig(f'SHAP_dependence_{le.classes_[i]}',bbox_inches='tight')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:49.987907Z","iopub.execute_input":"2023-09-23T03:48:49.988334Z","iopub.status.idle":"2023-09-23T03:48:53.67631Z","shell.execute_reply.started":"2023-09-23T03:48:49.988303Z","shell.execute_reply":"2023-09-23T03:48:53.675155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model with fewer features\n\nWe will now consider a model with the reduced number of features. The selected features occur in the set of top 10 features for atleast one of the three classes. It turns out this model performs slightly better than the previous baseline.","metadata":{}},{"cell_type":"code","source":"reduced_features = set()\nfor i in range(3):\n    reduced_features = reduced_features.union(\n        set(avg_shap_class[i].iloc[-8:].index.tolist())\n    )\n    \nreduced_features = list(reduced_features)\nprint(f'Number of feature selected: {len(reduced_features)}')\nprint('List of selected features:')\nprint(reduced_features)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:48:57.160394Z","iopub.execute_input":"2023-09-23T03:48:57.160802Z","iopub.status.idle":"2023-09-23T03:48:57.168502Z","shell.execute_reply.started":"2023-09-23T03:48:57.160772Z","shell.execute_reply":"2023-09-23T03:48:57.167695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=4, random_state=1)\ncv_f1_reduced_scores = [None]*40\nfor i, (train_index, test_index) in tqdm(enumerate(cv.split(X,y))):\n    cv_f1_reduced_scores[i] = fit_and_test_fold(X[reduced_features], y, train_index, test_index)\n\ncv_f1_reduced = np.mean(cv_f1_reduced_scores)\nprint(f'CV F1 for model with reduced number of features: {cv_f1_reduced:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:51:46.561548Z","iopub.execute_input":"2023-09-23T03:51:46.561899Z","iopub.status.idle":"2023-09-23T03:53:59.61851Z","shell.execute_reply.started":"2023-09-23T03:51:46.561874Z","shell.execute_reply":"2023-09-23T03:53:59.617699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CV F1 for each replicate of 10-fold CV\n# Clearly, there is some variability across replicates\nnp.array(cv_f1_reduced_scores).reshape(-1,10).mean(-1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:53:59.620568Z","iopub.execute_input":"2023-09-23T03:53:59.621347Z","iopub.status.idle":"2023-09-23T03:53:59.628532Z","shell.execute_reply.started":"2023-09-23T03:53:59.621315Z","shell.execute_reply":"2023-09-23T03:53:59.62732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_reduced = fit_model(X[reduced_features], y, n_jobs=4, verbose=50, random_seed=12)\n# save models to disk\nmodel_reduced.save_model('reduced_feats.cbm',format='cbm')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T03:53:59.629636Z","iopub.execute_input":"2023-09-23T03:53:59.629935Z","iopub.status.idle":"2023-09-23T03:54:03.127225Z","shell.execute_reply.started":"2023-09-23T03:53:59.62991Z","shell.execute_reply":"2023-09-23T03:54:03.126218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test predictions","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id':test.index.values,\n    'outcome':le.inverse_transform(model.predict(test).ravel())\n})\nsubmission.to_csv('submission_orig.csv',index=False)\nsubmission['outcome'].value_counts()/submission.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T23:59:14.939484Z","iopub.execute_input":"2023-09-21T23:59:14.939859Z","iopub.status.idle":"2023-09-21T23:59:14.959408Z","shell.execute_reply.started":"2023-09-21T23:59:14.939828Z","shell.execute_reply":"2023-09-21T23:59:14.958594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id':test.index.values,\n    'outcome':le.inverse_transform(model_reduced.predict(test[reduced_features]).ravel())\n})\nsubmission.to_csv('submission_reduced.csv',index=False)\nsubmission['outcome'].value_counts()/submission.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T23:59:16.495104Z","iopub.execute_input":"2023-09-21T23:59:16.495485Z","iopub.status.idle":"2023-09-21T23:59:16.51452Z","shell.execute_reply.started":"2023-09-21T23:59:16.495457Z","shell.execute_reply":"2023-09-21T23:59:16.513272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}