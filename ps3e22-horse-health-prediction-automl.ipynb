{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThe objective of this classification task is to predict the health outcomes of horses based on their historical medical data. There are three potential outcomes: \"lived,\" \"died,\" and \"euthanized.\" In this notebook, I will use AutoML via FLaML to simultaneously tune the hyperparameters of multiple supervised learning models and pick the best model. The preprocessing and feature engineering steps, including feature selection are taken from [this](https://www.kaggle.com/code/syerramilli/ps3e22-eda-catboost-baseline?scriptVersionId=143346482) notebook.","metadata":{}},{"cell_type":"code","source":"# issue with ray version 2.5: https://github.com/microsoft/FLAML/issues/1132\n!pip install FLAML \"ray[tune]<2.5.0\"","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T04:07:45.770651Z","iopub.execute_input":"2023-09-30T04:07:45.771054Z","iopub.status.idle":"2023-09-30T04:08:08.896957Z","shell.execute_reply.started":"2023-09-30T04:07:45.771020Z","shell.execute_reply":"2023-09-30T04:08:08.895579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom flaml import AutoML\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import f1_score\n\nfrom numbers import Number \nfrom pathlib import Path\nfrom typing import Optional, Dict\n\nplt.style.use('ggplot')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-30T04:08:08.899828Z","iopub.execute_input":"2023-09-30T04:08:08.900174Z","iopub.status.idle":"2023-09-30T04:08:14.657652Z","shell.execute_reply.started":"2023-09-30T04:08:08.900140Z","shell.execute_reply":"2023-09-30T04:08:14.656140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('/kaggle/input/playground-series-s3e22')\ntrain = pd.read_csv(path/'train.csv',index_col=['id'])\ntest = pd.read_csv(path/'test.csv',index_col=['id'])\n\ndel train['hospital_number']\ndel test['hospital_number']\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.658919Z","iopub.execute_input":"2023-09-30T04:08:14.659270Z","iopub.status.idle":"2023-09-30T04:08:14.746861Z","shell.execute_reply.started":"2023-09-30T04:08:14.659211Z","shell.execute_reply":"2023-09-30T04:08:14.745266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing (not including missing value analysis)\n\nAs mentioned earlier, I will be using the preprocessing and cleaning steps taken in the catboost notebook. The steps are\n\n1. Dropping the `lesion_2`, `lesion_3` and `Age` features: In each case, the most common value occurs in more than 90% of the observations. It is unlikely that any model can learn the effects of these features on the response.\n2. Decoding the site and type from `lesion_1` and then dropping `lesion_1`: Refer to the description in the [orignal dataset](https://www.kaggle.com/datasets/yasserh/horse-survival-dataset) for the details.\n3. Replacing erroneous categories in a couple of features with `pd.NA`\n4. For some of the categorical features, mapping a few categories with very few observations onto other categories\n5. Ordinal encoding columns that are either binary valued or have some inherent order\n6. Converting the `dtype` of the remaining categorical columns to `pd.Categorical`\n7. Creating a new numerical feature `log_pulseSq_total_protein` which is given by\n$$\\texttt{log_pulseSq_total_protein} = \\log\\left(\\frac{\\texttt{pulse}^2}{\\texttt{total_protein}}\\right):$$\nThis feature turns out to be useful for predicting the \"died\" outcomes.\n","metadata":{}},{"cell_type":"code","source":"def map_lesion_site(value:str) -> str:\n    '''\n    Return the site of the lesion given its code\n    '''\n    if value[:2] == \"11\" and len(value) == 5:\n        return \"all_intestinal\"\n    elif value[0] == \"1\":\n        return \"gastric\"\n    elif value[0] == \"2\":\n        return \"sm_intestine\"\n    elif value[0] == \"3\":\n        return \"lg_colon\"\n    elif value[0] == \"4\":\n        return \"lg_colon_and_cecum\"\n    elif value[0] == \"5\":\n        return \"cecum\"\n    elif value[0] == \"6\":\n        return \"transverse_colon\"\n    elif value[0] == \"7\":\n        return \"retum_colon\"\n    elif value[0] == \"8\":\n        return \"uterus\"\n    elif value[0] == \"9\":\n        return \"bladder\"\n    elif value[0] == \"0\":\n        return \"none\"\n    else:\n        return \"ERROR\"\n    \ndef map_lesion_type(value:str) -> str:\n    '''\n    Returns the type of lesion given its code\n    '''\n    if value == '0':\n        return \"none\"\n    \n    value2 = value[2] if len(value)==5 else value[1]\n    \n    if value2 == '1':\n        return \"simple\"\n    elif value2 == '2':\n        return 'strangulation'\n    elif value2 == '3':\n        return 'inflammation'\n    elif value2 == '4':\n        return 'other'\n    \n    return 'ERROR'","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.748552Z","iopub.execute_input":"2023-09-30T04:08:14.748989Z","iopub.status.idle":"2023-09-30T04:08:14.761865Z","shell.execute_reply.started":"2023-09-30T04:08:14.748948Z","shell.execute_reply":"2023-09-30T04:08:14.760848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [\n    'mucous_membrane', 'abdomen','rectal_exam_feces', \n    'lesion_site', 'lesion_type' \n]\n\ndef preprocess(df:pd.DataFrame) -> None:\n    # drop lesion_2, lesion_3, age\n    df = df.drop(['age','lesion_2','lesion_3'],axis=1)\n    \n    # parsing the lesion sites and types\n    # TODO: parse subtypes\n    df['lesion_site'] = df['lesion_1'].astype(str).apply(map_lesion_site)\n    df['lesion_type'] = df['lesion_1'].astype(str).apply(map_lesion_type)\n    del df['lesion_1']    \n    \n    \n    # cleaning some of the categorical features\n    df['peristalsis'] = df['peristalsis'].replace('distend_small',pd.NA)\n    df['rectal_exam_feces'] = df['rectal_exam_feces'].replace('serosanguious',pd.NA)\n    \n    # merging some of the categories\n    df['capillary_refill_time'] = df['capillary_refill_time'].replace('3','more_3_sec')\n    df['pain'] = df['pain'].replace('slight','alert')\n    \n    \n    # encoding some of the categorical levels as ordinal\n    ordinal_and_binary_dict = {\n        'surgery': ['no','yes'], \n        'temp_of_extremities': ['cold','cool', 'normal', 'warm'], \n        'peripheral_pulse': ['absent','reduced', 'normal','increased'], \n        'pain':['alert', 'depressed', 'mild_pain', 'moderate', 'severe_pain', 'extreme_pain'],\n        'capillary_refill_time': ['less_3_sec', 'more_3_sec'], \n        'peristalsis': ['absent', 'hypomotile', 'normal', 'hypermotile'], \n        'abdominal_distention': ['none', 'slight', 'moderate', 'severe'], \n        'nasogastric_tube': ['none', 'slight', 'significant'], \n        'nasogastric_reflux': ['none','slight','less_1_liter', 'more_1_liter'], \n        'abdomo_appearance': ['serosanguious', 'cloudy', 'clear'], \n        'surgical_lesion': ['no', 'yes'], \n        'cp_data': ['no', 'yes']\n    }\n    \n    for column, levels in ordinal_and_binary_dict.items():\n        df[column] = df[column].replace({\n            level:i for i,level in enumerate(levels)\n        })\n    \n    # converting the dtypes for the remaining columns to pd.Categorical \n    for column in categorical_columns:\n        # useful for other featur\n        df[column] = df[column].astype('category')\n        \n    \n    # feature engineering\n    df['log_pulseSq_total_protein'] = -np.log(df['total_protein']) + 2*np.log(df['pulse'])\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.764188Z","iopub.execute_input":"2023-09-30T04:08:14.764612Z","iopub.status.idle":"2023-09-30T04:08:14.778941Z","shell.execute_reply.started":"2023-09-30T04:08:14.764577Z","shell.execute_reply":"2023-09-30T04:08:14.777665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess\ntrain = preprocess(train)\ntest = preprocess(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.780658Z","iopub.execute_input":"2023-09-30T04:08:14.781872Z","iopub.status.idle":"2023-09-30T04:08:14.846683Z","shell.execute_reply.started":"2023-09-30T04:08:14.781828Z","shell.execute_reply":"2023-09-30T04:08:14.845356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling missing values\n\nIn the cell below, I compute the fraction of missing values in each column. (Note: Columns with no missing values are excluded).","metadata":{}},{"cell_type":"code","source":"def filter_greater_than(series:pd.Series,threshold:Number) -> pd.Series:\n    '''\n    Returns series elements greater than threshold. This funtion can be\n    used with the .pipe methods\n    '''\n    return series[series>threshold]\n\ndef get_perc_missing(df:pd.DataFrame) -> pd.Series:\n    return (\n        (df.isnull().sum()/df.shape[0]*100)\n        .sort_values(ascending=False)\n        .pipe(filter_greater_than,threshold=0)\n        .round(3)\n    )\n\nperc_missing = get_perc_missing(train)\nperc_missing","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.848251Z","iopub.execute_input":"2023-09-30T04:08:14.848897Z","iopub.status.idle":"2023-09-30T04:08:14.864266Z","shell.execute_reply.started":"2023-09-30T04:08:14.848862Z","shell.execute_reply":"2023-09-30T04:08:14.862947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same columns have missing entries in the test set too, and the percentage of missing values in the test set is roughly the same. So, we will need a concrete imputation strategy.","metadata":{}},{"cell_type":"code","source":"# get the percentage of missing entries within each\n# column of the test set\nget_perc_missing(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.865737Z","iopub.execute_input":"2023-09-30T04:08:14.866080Z","iopub.status.idle":"2023-09-30T04:08:14.880020Z","shell.execute_reply.started":"2023-09-30T04:08:14.866053Z","shell.execute_reply":"2023-09-30T04:08:14.878376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputation strategy\n\n1. For `abdomen` and `rectal_exam_feces`, we add a new category called `\"missing\"` for the missing entries.\n2. For the remaining columns, we impute the missing value with the mode.","metadata":{}},{"cell_type":"code","source":"# for abdome and rectal_exam_feces, add a new category called missing\nfor column in ['abdomen','rectal_exam_feces']:\n    train[column] = train[column].astype('object').fillna('missing').astype('category')\n    test[column] = test[column].astype('object').fillna('missing').astype('category')\n    \n# for the remaining columns with missing values, impute with mode\nfor column in perc_missing.iloc[2:].index:\n    mode_col = train[column].mode().iloc[0]\n    train[column] = train[column].fillna(mode_col)\n    test[column] = test[column].fillna(mode_col)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.881781Z","iopub.execute_input":"2023-09-30T04:08:14.882276Z","iopub.status.idle":"2023-09-30T04:08:14.908388Z","shell.execute_reply.started":"2023-09-30T04:08:14.882239Z","shell.execute_reply":"2023-09-30T04:08:14.907550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection\n\nIn the catboost baseline notebook ([here](https://www.kaggle.com/code/syerramilli/ps3e22-eda-catboost-baseline#Model-with-fewer-features)), I found that selecting a subset of features slightly improved modeling performance. I will be using the same subset here.","metadata":{}},{"cell_type":"code","source":"reduced_features = [\n    'pain', 'total_protein', 'surgery', 'packed_cell_volume', 'lesion_type', 'abdomo_protein', \n    'lesion_site', 'mucous_membrane', 'nasogastric_reflux_ph', 'rectal_exam_feces', \n    'log_pulseSq_total_protein', 'abdomo_appearance', 'temp_of_extremities', 'respiratory_rate'\n]\n\n\nX = train[reduced_features]\nX_test = test[reduced_features]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:08:14.909480Z","iopub.execute_input":"2023-09-30T04:08:14.910021Z","iopub.status.idle":"2023-09-30T04:08:14.920023Z","shell.execute_reply.started":"2023-09-30T04:08:14.909990Z","shell.execute_reply":"2023-09-30T04:08:14.918333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AutoML via FLaML\n\nFLAML tunes both the type of estimator (e.g., xgboost, random forest, etc.) and the hyperparameters for each estimator simulataneosuly.\n\n**Note**: In the AUTOML settings, I pass \"ensemble\":True. This means that the final model will be a stacked ensemble of the best models for each class.","metadata":{}},{"cell_type":"code","source":"automl = AutoML()\nautoml_settings = {\n    \"time_budget\": 1500,  # total running time in seconds (25 minutes)\n    \"metric\": 'micro_f1', \n    \"task\": 'classification',  # task type\n    \"estimator_list\":['lgbm', 'rf','xgboost', 'extra_tree', 'xgb_limitdepth'],\n    \"log_file_name\": 'health_outcomes_.log',\n    \"log_training_metric\": True,  # whether to log training metric\n    \"keep_search_state\": True, # needed if you want to keep the cross validation information\n    \"eval_method\": \"cv\",\n    \"split_type\": RepeatedStratifiedKFold(n_splits=10, n_repeats=4, random_state=1),\n    \"ensemble\":True,\n}\n\n\nwith warnings.catch_warnings():\n    # skips deprecation warnings from xgboost\n    warnings.simplefilter(\"ignore\")\n    automl.fit(X, train['outcome'], **automl_settings)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T04:12:07.482231Z","iopub.execute_input":"2023-09-30T04:12:07.482685Z","iopub.status.idle":"2023-09-30T04:12:28.498298Z","shell.execute_reply.started":"2023-09-30T04:12:07.482650Z","shell.execute_reply":"2023-09-30T04:12:28.497533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the best CV micro F1 scores for each estimator type.","metadata":{}},{"cell_type":"code","source":"# best loss per estimator\n(1-pd.Series(automl.best_loss_per_estimator)).sort_values(ascending=False).round(4)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:12:57.460405Z","iopub.execute_input":"2023-09-30T04:12:57.460894Z","iopub.status.idle":"2023-09-30T04:12:57.474137Z","shell.execute_reply.started":"2023-09-30T04:12:57.460851Z","shell.execute_reply":"2023-09-30T04:12:57.472757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get the corresponding configuration for each estimator. use the `.best_config_per_estimator` attribute","metadata":{}},{"cell_type":"code","source":"automl.best_config_per_estimator","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:12:28.508838Z","iopub.execute_input":"2023-09-30T04:12:28.509467Z","iopub.status.idle":"2023-09-30T04:12:28.524793Z","shell.execute_reply.started":"2023-09-30T04:12:28.509415Z","shell.execute_reply":"2023-09-30T04:12:28.523369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned earlier, the final model will be a stacked ensemble of the best models for each class of models. To disable this, and simply select the best performing model, set `\"ensemble\":False` in `automl_settings`.","metadata":{}},{"cell_type":"code","source":"automl.model","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:12:28.527701Z","iopub.execute_input":"2023-09-30T04:12:28.528408Z","iopub.status.idle":"2023-09-30T04:12:28.542370Z","shell.execute_reply.started":"2023-09-30T04:12:28.528368Z","shell.execute_reply":"2023-09-30T04:12:28.540762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model\nimport pickle\nwith open('automl_space_titanic.pkl', 'wb') as f:\n    pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:12:28.544206Z","iopub.execute_input":"2023-09-30T04:12:28.545214Z","iopub.status.idle":"2023-09-30T04:12:28.562049Z","shell.execute_reply.started":"2023-09-30T04:12:28.545171Z","shell.execute_reply":"2023-09-30T04:12:28.560524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test predictions","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id':test.index.values,\n    'outcome':automl.predict(X_test).ravel()\n})\nsubmission.to_csv('submission.csv',index=False)\nsubmission['outcome'].value_counts()/submission.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T04:12:31.543616Z","iopub.execute_input":"2023-09-30T04:12:31.544183Z","iopub.status.idle":"2023-09-30T04:12:31.613050Z","shell.execute_reply.started":"2023-09-30T04:12:31.544136Z","shell.execute_reply":"2023-09-30T04:12:31.611513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}